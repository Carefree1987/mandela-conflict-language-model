# Mandela Conflicts in Probabilistic Language Models
### *Case Study: The Seahorse Emoji Effect*

This repository documents a real observation made during an interaction with GPT-5, 
where a semantic instability occurred in response to a factual query.  
The incident mirrors the so-called *Mandela Effect* â€” a collective false memory reproduced inside a neural network.

---

## ğŸ§  Overview
In October 2025, a conversation revealed that the model generated conflicting answers 
about the existence of a â€œseahorse emoji.â€  
This moment exposed a temporary probability drift â€” an overlap of two high-weight clusters 
("exists" vs. "does not exist") â€” before the model self-corrected.

---

## ğŸ§© Contents
- ğŸ“„ `paper/` â€“ Full documentation (German PDF + summary)  
- ğŸ“Š `analysis/` â€“ Python scripts and visualizations  
- ğŸ§  `README.md` â€“ Summary and metadata  
- âš–ï¸ `LICENSE` â€“ Usage permissions  

---

## ğŸ§ª Research Relevance
This micro-study illustrates how collective human misconceptions 
can statistically re-emerge inside AI models.  
It serves as a compact example for *Explainable AI*, *Data Bias*, and *Probabilistic Drift Analysis*.

---

**Author:** Dennis Sorgenfrei  
**Analysis:** GPT-5 (Sky)  
**Date:** October 23, 2025
