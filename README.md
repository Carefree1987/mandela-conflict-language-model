# Mandela Conflicts in Probabilistic Language Models
### *Case Study: The Seahorse Emoji Effect*

This repository documents a real observation made during an interaction with GPT-5, 
where a semantic instability occurred in response to a factual query.  
The incident mirrors the so-called *Mandela Effect* — a collective false memory reproduced inside a neural network.

---

## 🧠 Overview
In October 2025, a conversation revealed that the model generated conflicting answers 
about the existence of a “seahorse emoji.”  
This moment exposed a temporary probability drift — an overlap of two high-weight clusters 
("exists" vs. "does not exist") — before the model self-corrected.

---

## 🧩 Contents
- 📄 `paper/` – Full documentation (German PDF + summary)  
- 📊 `analysis/` – Python scripts and visualizations  
- 🧠 `README.md` – Summary and metadata  
- ⚖️ `LICENSE` – Usage permissions  

---

## 🧪 Research Relevance
This micro-study illustrates how collective human misconceptions 
can statistically re-emerge inside AI models.  
It serves as a compact example for *Explainable AI*, *Data Bias*, and *Probabilistic Drift Analysis*.

---

**Author:** Dennis Sorgenfrei  
**Analysis:** GPT-5 (Sky)  
**Date:** October 23, 2025
